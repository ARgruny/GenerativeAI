{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PEFT technique:**\n",
    "\n",
    "I have chosen LoRA. LoRA is effective for adapting large pre-trained models with minimal additional parameters. It works by inserting low-rank matrices into existing weights, allowing for targeted updates without altering the entire model. It was especially useful to handle the restrictive GPU RAM. \n",
    "\n",
    "\n",
    "**Model:**\n",
    "\n",
    "The model selected is GPT-2 (124M), which is a smaller variant of the GPT-2 family, containing 124 million parameters. It's a good choice for experiments where computational resources might be a constraint, and it still retains strong language modeling capabilities.\n",
    "    \n",
    "**Evaluation approach:**\n",
    "\n",
    "The evaluation metrics I have chosen are accuracy, precision, recall, and F1 score. This comprehensive set of metrics will allow to assess the model's performance from various angles, evaluating its overall correctness (accuracy), its ability to correctly identify positive cases (precision and recall), and a balanced metric considering both precision and recall (F1 score).\n",
    "    \n",
    "**Fine-tuning dataset:**\n",
    "\n",
    "The IMDb dataset is selected for fine-tuning. This dataset, which contains movie reviews, is widely used for sentiment analysis tasks. It's an appropriate choice for fine-tuning a language model like GPT-2, especially when the aim is to enhance its capabilities in understanding and generating text related to movie reviews and sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.37.1\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__) # Current version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"gpt2\"\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"},\n",
    "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenize dataset\n",
    "splits =[\"train\", \"test\"]\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenized_ds = {}\n",
    "\n",
    "for split in splits:\n",
    "    tokenized_ds[split] = ds[split].map(\n",
    "        lambda x: tokenizer(\n",
    "            x[\"text\"], \n",
    "            truncation=True, \n",
    "            padding=\"max_length\", \n",
    "            return_tensors=\"pt\",\n",
    "            max_length=1024\n",
    "        ), batched=True \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to calculate metrics only using numpy\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    logits = np.array(logits)\n",
    "    labels = np.array(labels)\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    unique_labels = np.unique(labels)\n",
    "    cm = np.zeros((len(unique_labels), len(unique_labels)), dtype=int)\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        for j, pred in enumerate(unique_labels):\n",
    "            cm[i, j] = np.sum((labels == label) & (predictions == pred))\n",
    "\n",
    "            \n",
    "    eps = 0.000000001\n",
    "    tp = np.diag(cm) + eps\n",
    "    fp = np.sum(cm, axis=0) - tp + eps\n",
    "    fn = np.sum(cm, axis=1) - tp + eps\n",
    "\n",
    "    precision = np.mean(tp / (tp + fp))\n",
    "    recall = np.mean(tp / (tp + fn))\n",
    "    accuracy = np.mean(labels == predictions)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device) # Check if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used memory: 756926976 bytes\n"
     ]
    }
   ],
   "source": [
    "def print_cuda_memory_usage():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if device.type == 'cuda':\n",
    "        print(f\"Used memory: {torch.cuda.memory_allocated(device)} bytes\")\n",
    "    else:\n",
    "        print(\"CUDA is not available\")\n",
    "\n",
    "print_cuda_memory_usage() # Check memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ds_debug = tokenized_ds[\"test\"].train_test_split(test_size=0.01) # Split off a small debug set\n",
    "train_ds_debug = tokenized_ds[\"train\"].train_test_split(test_size=0.01) # Split off a small debug set\n",
    "model.config.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token) # Set pad token id for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using trainer class for evaluation of full test set\n",
    "training_args_eval = TrainingArguments(\n",
    "    output_dir=\"./outputs\",\n",
    "    do_train = False,\n",
    "    do_eval = True,\n",
    "    per_device_eval_batch_size=16,\n",
    ")\n",
    "\n",
    "eval_trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    args=training_args_eval,\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1615it [06:38,  4.05it/s]                          \n"
     ]
    }
   ],
   "source": [
    "results = eval_trainer.evaluate() # Evaluate model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.502259373664856,\n",
       " 'eval_accuracy': 0.4998,\n",
       " 'eval_f1': 0.4711475401793022,\n",
       " 'eval_precision': 0.44560212788392706,\n",
       " 'eval_recall': 0.49980000000004005,\n",
       " 'eval_runtime': 383.8466,\n",
       " 'eval_samples_per_second': 65.13,\n",
       " 'eval_steps_per_second': 4.072}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results # Print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used memory: 519954944 bytes\n"
     ]
    }
   ],
   "source": [
    "print_cuda_memory_usage() # Check memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import peft module\n",
    "import gc\n",
    "from peft import LoraConfig\n",
    "from peft import get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get model parameters with peft\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import TaskType\n",
    "# set config for peft\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 296448 || all params: 124737792 || trainable%: 0.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\ml1\\Lib\\site-packages\\peft\\tuners\\lora\\model.py:347: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# get model with peft\n",
    "model = get_peft_model(\n",
    "    model,\n",
    "    config,\n",
    ")\n",
    "\n",
    "print_trainable_parameters(model) # Print trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear memory if needed\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used memory: 756645888 bytes\n"
     ]
    }
   ],
   "source": [
    "print_cuda_memory_usage() # Check memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up trainer for training using peft model\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./training/data\", # The output directory\n",
    "        learning_rate=2e-5, # learning rate\n",
    "        per_device_train_batch_size=4, # batch size for training\n",
    "        per_device_eval_batch_size=2, # batch size for evaluation\n",
    "        evaluation_strategy=\"epoch\", # evaluate after each epoch\n",
    "        save_strategy=\"epoch\", # save after each epoch\n",
    "        num_train_epochs=3, # number of training epochs\n",
    "        weight_decay=0.01, # strength of weight decay\n",
    "        load_best_model_at_end=True, # load the best model when finished training (default metric is loss)\n",
    "        label_names=[\"labels\"] # name of the labels\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"], # training dataset\n",
    "    eval_dataset=eval_ds_debug[\"test\"],# used smaller debug set for evaluation during training\n",
    "    tokenizer=tokenizer, # tokenizer\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer), # data collator\n",
    "    compute_metrics=compute_metrics # metrics function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 501/18750 [01:21<49:30,  6.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8515, 'learning_rate': 1.9466666666666668e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1001/18750 [02:42<48:02,  6.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6485, 'learning_rate': 1.8933333333333334e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1501/18750 [04:02<46:39,  6.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4161, 'learning_rate': 1.8400000000000003e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 2001/18750 [05:23<45:20,  6.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3825, 'learning_rate': 1.7866666666666666e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2501/18750 [06:44<44:01,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4554, 'learning_rate': 1.7333333333333336e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 3001/18750 [08:05<42:46,  6.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4202, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 3501/18750 [09:26<41:21,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4648, 'learning_rate': 1.6266666666666668e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 4001/18750 [10:47<39:58,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.426, 'learning_rate': 1.5733333333333334e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 4501/18750 [12:08<38:37,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4546, 'learning_rate': 1.5200000000000002e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 5001/18750 [13:29<37:15,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4056, 'learning_rate': 1.4666666666666666e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 5501/18750 [14:50<35:52,  6.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4062, 'learning_rate': 1.4133333333333334e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 6001/18750 [16:11<34:37,  6.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3714, 'learning_rate': 1.3600000000000002e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 33%|███▎      | 6250/18750 [16:55<33:31,  6.21it/s]Checkpoint destination directory ./training/data\\checkpoint-6250 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      " 33%|███▎      | 6251/18750 [16:56<4:26:43,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5142213702201843, 'eval_accuracy': 0.896, 'eval_f1': 0.8995856237093031, 'eval_precision': 0.9012337704290222, 'eval_recall': 0.8979434941387991, 'eval_runtime': 3.6837, 'eval_samples_per_second': 67.867, 'eval_steps_per_second': 33.933, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 6501/18750 [17:36<32:17,  6.32it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4467, 'learning_rate': 1.3066666666666668e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 7001/18750 [18:55<30:59,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3904, 'learning_rate': 1.2533333333333336e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 7501/18750 [20:13<29:46,  6.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3884, 'learning_rate': 1.2e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 8001/18750 [21:32<28:20,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4354, 'learning_rate': 1.1466666666666668e-05, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 8501/18750 [22:51<27:05,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3874, 'learning_rate': 1.0933333333333334e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 9001/18750 [24:10<25:42,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.395, 'learning_rate': 1.04e-05, 'epoch': 1.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 9501/18750 [25:29<24:22,  6.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3901, 'learning_rate': 9.866666666666668e-06, 'epoch': 1.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 10001/18750 [26:48<23:02,  6.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3512, 'learning_rate': 9.333333333333334e-06, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 10501/18750 [28:07<21:47,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3643, 'learning_rate': 8.8e-06, 'epoch': 1.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 11001/18750 [29:26<20:29,  6.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4414, 'learning_rate': 8.266666666666667e-06, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 11501/18750 [30:45<19:06,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3772, 'learning_rate': 7.733333333333334e-06, 'epoch': 1.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 12001/18750 [32:04<17:48,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3788, 'learning_rate': 7.2000000000000005e-06, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 12500/18750 [33:23<16:24,  6.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3611, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 67%|██████▋   | 12500/18750 [33:26<16:24,  6.35it/s]Checkpoint destination directory ./training/data\\checkpoint-12500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      " 67%|██████▋   | 12501/18750 [33:27<2:09:49,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4192880392074585, 'eval_accuracy': 0.908, 'eval_f1': 0.9083501943229663, 'eval_precision': 0.9081541218645328, 'eval_recall': 0.908546351464627, 'eval_runtime': 3.5875, 'eval_samples_per_second': 69.687, 'eval_steps_per_second': 34.843, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 13001/18750 [34:46<15:10,  6.32it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3586, 'learning_rate': 6.133333333333334e-06, 'epoch': 2.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 13501/18750 [36:04<13:50,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3802, 'learning_rate': 5.600000000000001e-06, 'epoch': 2.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 14001/18750 [37:23<12:32,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3954, 'learning_rate': 5.0666666666666676e-06, 'epoch': 2.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 14501/18750 [38:42<11:12,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3475, 'learning_rate': 4.533333333333334e-06, 'epoch': 2.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 15001/18750 [40:01<09:53,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3907, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 15501/18750 [41:20<08:34,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3561, 'learning_rate': 3.4666666666666672e-06, 'epoch': 2.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 16001/18750 [42:39<07:14,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3567, 'learning_rate': 2.9333333333333338e-06, 'epoch': 2.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 16501/18750 [43:58<05:56,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.419, 'learning_rate': 2.4000000000000003e-06, 'epoch': 2.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 17001/18750 [45:17<04:36,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3543, 'learning_rate': 1.8666666666666669e-06, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 17501/18750 [46:36<03:17,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3718, 'learning_rate': 1.3333333333333334e-06, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 18001/18750 [47:55<01:58,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4005, 'learning_rate': 8.000000000000001e-07, 'epoch': 2.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 18501/18750 [49:14<00:39,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3499, 'learning_rate': 2.666666666666667e-07, 'epoch': 2.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      "100%|██████████| 18750/18750 [49:56<00:00,  6.38it/s]Checkpoint destination directory ./training/data\\checkpoint-18750 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "100%|██████████| 18750/18750 [49:56<00:00,  6.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41768962144851685, 'eval_accuracy': 0.912, 'eval_f1': 0.9122111113490372, 'eval_precision': 0.912000000000704, 'eval_recall': 0.9124223204568451, 'eval_runtime': 3.5688, 'eval_samples_per_second': 70.051, 'eval_steps_per_second': 35.025, 'epoch': 3.0}\n",
      "{'train_runtime': 2996.9875, 'train_samples_per_second': 25.025, 'train_steps_per_second': 6.256, 'train_loss': 0.41210360310872396, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18750, training_loss=0.41210360310872396, metrics={'train_runtime': 2996.9875, 'train_samples_per_second': 25.025, 'train_steps_per_second': 6.256, 'train_loss': 0.41210360310872396, 'epoch': 3.0})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train() # train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:03<00:00, 34.43it/s]\n"
     ]
    }
   ],
   "source": [
    "result = trainer.evaluate() # evaluate model on debug test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.41768962144851685,\n",
       " 'eval_accuracy': 0.912,\n",
       " 'eval_f1': 0.9122111113490372,\n",
       " 'eval_precision': 0.912000000000704,\n",
       " 'eval_recall': 0.9124223204568451,\n",
       " 'eval_runtime': 3.8528,\n",
       " 'eval_samples_per_second': 64.888,\n",
       " 'eval_steps_per_second': 32.444,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result # print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model\n",
    "model.save_pretrained(\"./models/gpt2-lora\")\n",
    "trainer.save_model(\"./models/gpt2-lora-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload() # merge peft model with original model. Not needed for inference. makes model footprint larger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed modules for inference\n",
    "from peft import PeftConfig, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load pretrained model with peft\n",
    "peft_model_path = \"./models/gpt2-lora-v2\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(peft_model_path) # load config\n",
    "\n",
    "# load base model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    label2id={\"NEGATIVE\": 0, \"POSITIVE\": 1},\n",
    "    id2label={0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    ")\n",
    "\n",
    "# load peft model and merge with base model\n",
    "inf_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    peft_model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_model.config.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token) # set pad token id for model\n",
    "inf_model.eval() # set model to evaluation mode\n",
    "\n",
    "\n",
    "## use trainer class for evaluation of full test set\n",
    "training_args_eval = TrainingArguments(\n",
    "    output_dir=\"./outputs\",\n",
    "    do_train = False,\n",
    "    do_eval = True,\n",
    "    per_device_eval_batch_size=16,\n",
    "    label_names=[\"labels\"]\n",
    ")\n",
    "\n",
    "eval_trainer = Trainer(\n",
    "    model=inf_model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    args=training_args_eval,\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [06:48<00:00,  3.83it/s]\n"
     ]
    }
   ],
   "source": [
    "results = eval_trainer.evaluate() # evaluate model on full test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.37846362590789795,\n",
       " 'eval_accuracy': 0.9214,\n",
       " 'eval_f1': 0.9216021099276379,\n",
       " 'eval_precision': 0.9218043085407156,\n",
       " 'eval_recall': 0.9214000000000062,\n",
       " 'eval_runtime': 408.832,\n",
       " 'eval_samples_per_second': 61.15,\n",
       " 'eval_steps_per_second': 3.823}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results # print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted label 1: POSITIVE\n",
      "predicted label 2: NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "## example inference and label prediction\n",
    "# set up input for positive and negative example\n",
    "inputs_1 = tokenizer(\n",
    "    \"I love this movie!\",\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=1024\n",
    ")\n",
    "\n",
    "inputs_2 = tokenizer(\n",
    "    \"I hate this movie!\",\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=1024\n",
    ")\n",
    "\n",
    "# move inputs to device\n",
    "inputs_1.to(device)\n",
    "inputs_2.to(device)\n",
    "inf_model.to(device)\n",
    "\n",
    "# inference\n",
    "outputs_1 = inf_model(**inputs_1)\n",
    "outputs_2 = inf_model(**inputs_2)\n",
    "logits_1 = outputs_1.logits\n",
    "logits_2 = outputs_2.logits\n",
    "probabilities_1 = F.softmax(logits_1, dim=-1)\n",
    "probabilities_2 = F.softmax(logits_2, dim=-1)\n",
    "predicted_class_idx_1 = torch.argmax(probabilities_1, dim=-1)\n",
    "predicted_class_idx_2 = torch.argmax(probabilities_2, dim=-1)\n",
    "\n",
    "predicted_class_label_1 = inf_model.config.id2label[predicted_class_idx_1.item()]\n",
    "predicted_class_label_2 = inf_model.config.id2label[predicted_class_idx_2.item()]\n",
    "\n",
    "# print results\n",
    "print(f\"predicted label 1: {predicted_class_label_1}\")\n",
    "print(f\"predicted label 2: {predicted_class_label_2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
